{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782cf6c6-0822-4cc1-a06c-5534aac3265a",
   "metadata": {},
   "source": [
    "<font color=#4242f5>\n",
    "        <h1><center><strong>CME466</strong></center></h1>\n",
    "        <h1><center><strong>Design of an Advanced Digital System</strong></center></h1>\n",
    "        <p><center><strong>Department of Electrical and Computer Engineering</strong></center></p>\n",
    "        <p><center><strong>University of Saskatchewan</strong></center></p>\n",
    "        <p><center><strong>2024 Winter Term</strong></center></p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4630b-b344-41a6-aaf9-378b16dc3fb6",
   "metadata": {},
   "source": [
    "<h1><font color = #f5bc42><strong>Imagine a vampire that knows programming ...</strong></font></h1>\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../data/batman.jpg\" alt=\"Fun Meme\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b482271",
   "metadata": {},
   "source": [
    "# <h1> <font color=#ad42f5><strong>Module1: Introduction to Image Processing with OpenCV</strong></font></h1>\n",
    "\n",
    "**Instructors:** \n",
    "- Dr. Khan Wahid (khan.wahid@usask.ca)\n",
    "- Mokarrameh Einlou (mokarrameh.einlou@usask.ca)\n",
    "\n",
    "Materials prepared by Mokarrameh Einlou (mokarrameh.einlou@usask.ca)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9e28d-8fe1-4156-95bd-99e69a2fe2c2",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.1 Why OpenCV?</strong></font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeaee4-b2ad-4a04-b066-a4edcd9a49b9",
   "metadata": {},
   "source": [
    "> <strong><span style=\"color:#2ecc71;\">\"If you can build a simple rule-based system that doesn't require machine learning, do that.\"</span></strong>  \n",
    "> <strong><span style=\"color:#9b59b6;\">\\- Rule one of Google's Machine Learning Handbook</span></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d266ef-48c1-4f93-89fe-04fd03230fce",
   "metadata": {},
   "source": [
    "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Being an Apache 2 licensed product, OpenCV makes it easy for businesses to utilize and modify the code.\n",
    "\n",
    "The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc. OpenCV has more than 47 thousand people of user community and estimated number of downloads exceeding 18 million. The library is used extensively in companies, research groups and by governmental bodies.\n",
    "\n",
    "> Click [Here](https://opencv.org) For More Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c9529-2537-4002-a903-7c4b0257977d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2><font color=#f542f2><strong>1.2 What are images?</strong></font></h2>\n",
    "\n",
    "Images are numpy arrays!\n",
    "\n",
    "> **Note:**\n",
    "> NumPy is a Python library for numerical computing. It provides support for multidimensional arrays and matrices, along with mathematical functions to operate on them. The core data structure is the NumPy array (numpy.ndarray), allowing efficient numerical operations. NumPy is widely used in scientific computing, machine learning, and data science. To explore the full capabilities of NumPy, refer to the NumPy [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f0b25-eed6-458c-a26c-bbb1469d8dba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<h3><font color=#4296f5><strong>1.2.1 Anatomy of Digital Images</strong></font></h3>\n",
    "\n",
    "Images are made of pixels, and each pixel represents the smallest unit of information in a digital image. The composition of an image involves arranging these pixels in a grid to create a visual representation. Let's explore what images are made of:\n",
    "\n",
    "<h4><font color=#42f590><strong>Pixels</strong></font></h4>\n",
    "\n",
    "- **Definition:** Pixels, or \"picture elements,\" are the building blocks of digital images.\n",
    "- **Attributes:**\n",
    "  - Color: RGB values (Red, Green, Blue) in color images.\n",
    "  - Intensity: Grayscale images have a single intensity value per pixel.\n",
    "- **Coordinate System:** Pixels are arranged in a 2D grid with (0, 0) at the top-left corner.\n",
    "\n",
    "<h4><font color=#42f590><strong>Color Channels</strong></font></h4>\n",
    "\n",
    "- **RGB Model:** Color images consist of three channels—Red, Green, Blue.\n",
    "- **Grayscale:** Single-channel images represent light intensity (0 to 255).\n",
    "\n",
    "<h4><font color=#42f590><strong>Resolution</strong></font></h4>\n",
    "\n",
    "- **Definition:** Resolution is the number of pixels in the width and height of an image.\n",
    "- **Quality:** Higher resolution provides finer details.\n",
    "\n",
    "<h4><font color=#42f590><strong>Color Depth</strong></font></h4>\n",
    "\n",
    "- **Definition:** Color depth refers to the number of bits used to represent colors in each channel.\n",
    "- **Common Depths:** 8-bit (256 colors), 16-bit, 24-bit (true color).\n",
    "\n",
    "<h4><font color=#42f590><strong>Coordinate System</strong></font></h4>\n",
    "\n",
    "- **Origin:** (0, 0) is at the top-left corner.\n",
    "- **Axes:** Horizontal (x) increases left to right; Vertical (y) increases top to bottom.\n",
    "\n",
    "<h4><font color=#42f590><strong>Metadata</strong></font></h4>\n",
    "\n",
    "- **Definition:** Images may contain metadata such as creation date, camera settings, etc.\n",
    "- **Storage:** Embedded within the image file.\n",
    "\n",
    "<h4><font color=#42f590><strong>Compression</strong></font></h4>\n",
    "\n",
    "- **Purpose:** Reduce file size for efficient storage.\n",
    "- **Formats:** JPEG, PNG, GIF, etc.\n",
    "\n",
    "<h4><font color=#42f590><strong>Alpha Channel</strong></font></h4>\n",
    "\n",
    "- **Definition:** Represents pixel transparency.\n",
    "- **Usage:** Allows for images with varying levels of opacity.\n",
    "\n",
    "<h4><font color=#42f590><strong>Image Formats</strong></font></h4>\n",
    "\n",
    "- **Storage:** Images are stored in various formats (JPEG, PNG, BMP, TIFF).\n",
    "- **Characteristics:** Each format has unique features and compression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26063763-a00a-4cfc-8720-69186cf846d4",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.3. Installing OpenCV</strong></font></h2>\n",
    "<h3><font color=#4296f5><strong>1.3.1 Anaconda Environment</strong></font></h3>\n",
    "\n",
    "There are some difficulties installing OpenCV using the ```conda install``` command. So, I recommend using the GUI (just this time!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ac9f8-2efe-44a1-bc90-8e6af1b09efe",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.3.2 On an Ubuntu machine</strong></font></h3>\n",
    "Follow the steps below to set up OpenCV.\n",
    "\n",
    "1. **Open a Terminal:**\n",
    "\n",
    "2. **Install OpenCV:**\n",
    "   - In the terminal, type the following command and press Enter:\n",
    "     ```bash\n",
    "     sudo apt install python3-opencv\n",
    "     ```\n",
    "\n",
    "3. **Proceed with Installation:**\n",
    "   - You will be prompted to confirm the installation. Type 'y' and press Enter to proceed.\n",
    "\n",
    "4. **Verify Installation:**\n",
    "   - After the installation is complete, you can verify it by importing OpenCV in a Python script or Jupyter Notebook:\n",
    "     ```python\n",
    "     import cv2\n",
    "     print(cv2.__version__)\n",
    "     ```\n",
    "\n",
    "Congratulations! You've successfully installed OpenCV. You can now use it for various image processing tasks.\n",
    "\n",
    "**Note:** If you encounter any issues or want more customization, refer to the [official OpenCV documentation](https://docs.opencv.org/master/da/df6/tutorial_py_table_of_contents_setup.html) for additional installation options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abc0f2-a294-492f-905b-a64846c2d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cv2 library\n",
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80c760-017d-455b-8b19-81512adfaa71",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.4 Image Read, Write, and Visualization with OpenCV</strong></font></h2>\n",
    "\n",
    ">**Note**\n",
    ">    Click [here](https://docs.opencv.org/4.x/db/deb/tutorial_display_image.html) for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f965eb-178d-4913-9ac8-8a6f657316c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read image\n",
    "image_path = os.path.join('..', 'data', 'sample2.jpg')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# write image\n",
    "cv2.imwrite(os.path.join('..', 'data', 'sample1_copy.jpg'), img)\n",
    "\n",
    "# This line opens a pop up window\n",
    "# visualize images\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows() simply destroys all the windows we created.\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),plt.title('Original Image')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea80fa-0cf2-40dd-b428-1e39fcd15bc4",
   "metadata": {},
   "source": [
    "This script performs the following operations:\n",
    "\n",
    "1. **Import Libraries:** Imports necessary libraries, including os for working with file paths and cv2 for OpenCV.\n",
    "\n",
    "2. **Define Image Path:** Constructs the path to the input image file (sample1.jpg).\n",
    "\n",
    "3. **Read Image:** Uses cv2.imread() to read the input image into a NumPy array (img).\n",
    "\n",
    "4. **Write Image Copy:** Writes a copy of the image to a new file (sample1_copy.jpg).\n",
    "\n",
    "5. **Visualize Image:** Displays the original image in an OpenCV window titled 'image' and waits for a key press.\n",
    "\n",
    "6. **Close Windows:** Closes all OpenCV windows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c3d14-4e04-4061-a18f-6731cd448ecf",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.4.1 Image Shape</strong></font></h3>\n",
    "\n",
    "Shape method is typically used to determine the dimensions of an image. The output of the shape method on an image is a tuple representing the number of rows, columns, and channels (if it's a color image).\n",
    "\n",
    "For a grayscale image, the tuple would contain two elements:\n",
    "(height, width)\n",
    "\n",
    "and for a color image, the tuple would contain three elements:\n",
    "(height, width, channels)\n",
    "\n",
    "Here, height represents the number of rows, width represents the number of columns, and channels represents the number of color channels (e.g., 3 for RGB images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017f404-6d86-4c4e-bfea-a917ae4b6e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf68a5-c7eb-45b0-8d6c-2b1e2d792691",
   "metadata": {},
   "source": [
    "- **In \"most cases\" pixel value range from 0 to 255**\n",
    "- **In binary images pixel value is in 0 and 1 (or 0 and 255)**\n",
    "\n",
    "  Every pixel is either black or white.\n",
    "- **In 16 bit images pixel value range from 0 to 65535.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# show Blue values\n",
    "cv2.imshow('image-B', img[:,:,0])\n",
    "# show Green values\n",
    "cv2.imshow('image-G', img[:,:,1])\n",
    "# show Red values\n",
    "cv2.imshow('image-R', img[:,:,2])\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show in colour values\n",
    "\n",
    "b = img.copy()\n",
    "# set green and red channels to 0\n",
    "b[:, :, 1] = 0\n",
    "b[:, :, 2] = 0\n",
    "\n",
    "\n",
    "g = img.copy()\n",
    "# set blue and red channels to 0\n",
    "g[:, :, 0] = 0\n",
    "g[:, :, 2] = 0\n",
    "\n",
    "r = img.copy()\n",
    "# set blue and green channels to 0\n",
    "r[:, :, 0] = 0\n",
    "r[:, :, 1] = 0\n",
    "\n",
    "\n",
    "# RGB - Blue\n",
    "cv2.imshow('B-RGB', b)\n",
    "\n",
    "# RGB - Green\n",
    "cv2.imshow('G-RGB', g)\n",
    "\n",
    "# RGB - Red\n",
    "cv2.imshow('R-RGB', r)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "y, u, v = cv2.split(img_yuv)\n",
    "\n",
    "cv2.imshow('y', y)\n",
    "cv2.imshow('u', u)\n",
    "cv2.imshow('v', v)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c553c",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.10 Color Spaces</strong></font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730f844",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "    In this tutorial, you will learn how to convert images from one color-space to another, like BGR ↔ Gray, BGR ↔ HSV, etc.\n",
    "> Every time you read an image using the OpenCV library, it'll be in the BGR color space. It means that every single pixel is a combination of three colors: Blue, Green, and Red.\n",
    "\n",
    "[Changing Colorspaces](https://docs.opencv.org/3.4/df/d9d/tutorial_py_colorspaces.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d7398",
   "metadata": {},
   "source": [
    "There are more than 150 color-space conversion methods available in OpenCV. But we will look into only three: BGR ↔ RGB, BGR ↔ Gray, and BGR ↔ HSV.\n",
    "\n",
    "For color conversion, we use the function cv.cvtColor(input_image, flag) where flag determines the type of conversion.\n",
    "\n",
    "For BGR → Gray conversion, we use the flag cv.COLOR_BGR2GRAY. Similarly for BGR → HSV, we use the flag cv.COLOR_BGR2HSV. To get other flags, just run following commands in your Python terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b3550-70f8-4775-8a4c-fa01dc4b5971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "flags = [i for i in dir(cv) if i.startswith('COLOR_')]\n",
    "print( flags )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c37b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'data', 'sunder.jpg')\n",
    "#image_path = os.path.join ('..', 'data', 'miguel.jpg')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "    \n",
    "cv2.imshow('BGR image', img)\n",
    "print(img.shape)\n",
    "\n",
    "img_rgb = cv2.cvtColor (img, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow ('RGB image', img_rgb)\n",
    "print(img_rgb.shape)\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow ('Gray image', img_gray)\n",
    "print(img_gray.shape)\n",
    "\n",
    "img_hsv = cv2.cvtColor (img, cv2.COLOR_BGR2HSV)\n",
    "cv2.imshow ('HSV image', img_hsv)\n",
    "print(img_hsv.shape)\n",
    "\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab33d2",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.5 Image Histogram</strong></font></h3>\n",
    "\n",
    "Histogram is considered as a graph or plot which is related to frequency of pixels in an Gray Scale Image with pixel values (ranging from 0 to 255). \n",
    "\n",
    "Grayscale image is an image in which the value of each pixel is a single sample, that is, it carries only intensity information where pixel value varies from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd64a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join('..', 'data', 'sample2.jpg')\n",
    "# read image in grey scale\n",
    "img = cv2.imread (image_path, 0)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "histr = cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "plt.plot(histr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the pixel intensities for YUV\n",
    "plt.figure().set_figwidth(15)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(u.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(v.ravel(),256,[0,256]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609270b9-1198-43d5-beff-24dce43797cc",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.5 Read, Write, and Display a Video Using OpenCV (Without Sound)</strong></font></h2>\n",
    "\n",
    "> **what is a video?** A video is a sequence of fast moving images. The obvious question that follows is how fast are the pictures moving? The measure of how fast the images are transitioning is given by a metric called **frames per second(FPS)**.\n",
    "\n",
    "> When someone says that the video has an FPS of 40, it means that 40 images are being displayed every second. Alternatively, after every 25 milliseconds, a new frame is displayed. The other important attributes are the width and height of the frame.\n",
    "\n",
    "> In OpenCV, a video can be read either by using the feed from a camera connected to a computer or by reading a video file. The first step towards reading a video file is to create a VideoCapture object. Its argument can be either the device index or the name of the video file to be read.\n",
    "\n",
    "> **Displaying the Video:** After reading a video file, we can display the video frame by frame. A frame of a video is simply an image and we display each frame the same way we display images, i.e., we use the function `cv2.imshow()`.\n",
    "\n",
    "> As in the case of an image, we use the `cv2.waitKey()` after `cv2.imshow()` function to pause each frame in the video. In the case of an image, we pass ‘0’ to the `cv2.waitKey()` function, but for playing a video, we need to pass a number greater than ‘0’ to the `cv2.waitKey()` function. This is because ‘0’ would pause the frame in the video for an infinite amount of time and in a video we need each frame to be shown only for some finite interval of time. So, we need to pass a number greater than ‘0’ to the `cv2.waitKey()` function. This number is equal to the time in milliseconds we want each frame to be displayed.\n",
    ">\n",
    "> While reading the frames from a webcam, using `cv2.waitKey(1)` is appropriate because the display frame rate will be limited by the frame rate of the webcam even if we specify a delay of 1 ms in waitKey.\n",
    "\n",
    "> While reading frames from a video that you are processing, it may still be appropriate to set the time delay to 1 ms so that the thread is freed up to do the processing we want to do.\n",
    "\n",
    "> In rare cases, when the playback needs to be at a certain framerate, we may want the delay to be higher than 1 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94357a5d-4960-4a16-b1e0-5554cc034dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "video_path = os.path.join('..', 'data', 'BR99.mp4')\n",
    "\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (video_cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Get video details\n",
    "fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "ft = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while video_cap.isOpened():\n",
    "    # Reading frames of the video\n",
    "    ret, frame = video_cap.read ()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close all windows\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec98e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fps)\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "video_path = os.path.join('..', 'data', 'BR99.mp4')\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "video_cap.set(cv2.CAP_PROP_POS_FRAMES, 10)\n",
    "ret, frame = video_cap.read()\n",
    "cv2.imshow('frame captured', frame)\n",
    "\n",
    "while True:\n",
    "    ch = 0xFF & cv2.waitKey(1) # Wait for a second\n",
    "    if ch == ord('q'):\n",
    "        break\n",
    "\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95333d4f-8236-4086-ae00-1b4d33d10bfb",
   "metadata": {},
   "source": [
    "- `video_cap.read()` returns a bool (True/False). If the frame is read correctly, it will be True. So you can check for the end of the video by checking this returned value.\n",
    "\n",
    "- Sometimes, video_cap may not have initialized the capture. You can check whether it is initialized or not by the method `video_cap.isOpened()`. If it is True, OK. Otherwise open it using `video_cap.open()`.\n",
    "\n",
    "- You can also access some of the features of this video using `video_cap.get(propId)` method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Full details can be seen [here](https://docs.opencv.org/4.x/d8/dfe/classcv_1_1VideoCapture.html#aa6480e6972ef4c00d74814ec841a2939). Some of these values can be modified using `video_cap.set(propId, value)`. Value is the new value you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0edf9-d8bb-4156-8616-ea19b0cfdeab",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.6 Read, Write, and Display a Video Using OpenCV (With Sound)</strong></font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587bc90-69a1-47a9-af20-2c5687a46548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "# ffpyplayer for playing audio\n",
    "# https://matham.github.io/ffpyplayer/installation.html\n",
    "# pip install ffpyplayer\n",
    "\n",
    "from ffpyplayer.player import MediaPlayer\n",
    "\n",
    "video_path = os.path.join('..', 'data', 'BR99.mp4')\n",
    "\n",
    "video=cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (video.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Get video details\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "player = MediaPlayer(video_path)\n",
    "while video.isOpened():\n",
    "    grabbed, frame=video.read()\n",
    "    audio_frame, val = player.get_frame()\n",
    "    if not grabbed:\n",
    "        print(\"End of video\")\n",
    "        break\n",
    "    if cv2.waitKey(int(1000 / fps)) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    if val != 'eof' and audio_frame is not None:\n",
    "        #audio\n",
    "        img, t = audio_frame\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3adc75-0f56-486c-a11e-2ee7b6a06ebd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2><font color=#f542f2><strong>1.7 Accessing The Webcam Using OpenCV</strong></font></h2>\n",
    "\n",
    ">To capture a video, you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cfd56f-727e-4f74-b82e-a2a8ac8018d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    # Our operations on the frame come here\n",
    "    # For example we can convert the frames into gray color-space\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    cv.imshow('frame', gray)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cab585-7857-48e7-bd2e-486e962d5653",
   "metadata": {},
   "source": [
    "> **Q: Why 40?**\n",
    "> When you are reading frames from your webcam the process is slightlkyu slower from when you are reading frames of a video stored locally on your computer.\n",
    "> So, calculating the amount of time you need to wait for each fram, is a bit complicated. And it really depends on the amount of time it takes for the webcam to read the frame and the amount of frames per second you are getting from the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c694aaf-2ae4-4c2c-9bdb-748bea684fd0",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.8 Basic Operations</strong></font></h2>\n",
    "<h3><font color=#4296f5><strong>1.8.1 Resizing</strong></font></h3>\n",
    "\n",
    "To resize an image, scale it along each axis (height and width), considering the specified scale factors or just set the desired height and width.  \n",
    "\n",
    "**When resizing an image:**\n",
    "- It is important to keep in mind the original aspect ratio of the image (i.e. width by height), if you want to maintain the same in the resized image too.\n",
    "- Reducing the size of an image will require resampling of the pixels. \n",
    "- Increasing the size of an image requires reconstruction of the image. This means you need to interpolate new pixels.\n",
    "- Various interpolation techniques come into play to accomplish these operations. Several methods are available in OpenCV, the choice typically depends on the particular application.\n",
    "\n",
    "Click [here](https://learnopencv.com/image-resizing-with-opencv/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff90eac-0a18-44b4-9fca-b89d1be9c791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'sample2.jpg')\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# check for errors\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Printing the shape of the image (height, width, channels)\n",
    "print(img.shape)\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "# Resizing the image\n",
    "resized_img = cv2.resize (img, (int(img.shape[1]/2), int(img.shape[0]/2)))\n",
    "\n",
    "print(resized_img.shape)\n",
    "\n",
    "cv2.imshow('resized image', resized_img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f25990-1f4b-4453-8304-6570bcfe1ee7",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.8.2 Cropping</strong></font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92153323-fa0a-4e57-9d91-15d787d6dd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'fedor.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# check for errors\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Printing the shape of the image (height, width, channels)\n",
    "print(img.shape)\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "# image is just a numpy array. So, to crop it we can just use slicing techniques\n",
    "cropped_img = img[600:1050, 150:1250]\n",
    "cv2.imshow('cropped image', cropped_img)\n",
    "print(cropped_img.shape)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfb00b-5cda-4a91-85a5-1e1802d42c4d",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.9 Image Enhancement Techniques</strong></font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead0f51-8d49-4118-acdd-9a24c493647e",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.9.1 Adjusting brightness and contrast</strong></font></h3>\n",
    "\n",
    "Adjusting the brightness and contrast of an image can significantly affect its visual appeal and effectiveness. It can also help to correct defects or flaws in the image and make it easier to see details. Finding the right balance of brightness and contrast is important for creating an attractive and effective image.\n",
    "\n",
    "There are several ways to adjust the brightness and contrast of an image using OpenCV and Python. One common method is to use the `cv2.addWeighted()` function, which allows you to adjust the brightness by adding a scalar value to each pixel in the image, and the contrast by scaling the pixel values.\n",
    "Here is an example of how to adjust the brightness and contrast of an image using the `cv2.addWeighted()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379a566-dc36-48ab-a8a7-0b97fa4894ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'batman1.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "  \n",
    "# Adjust the brightness and contrast \n",
    "# Adjusts the brightness by adding 10 to each pixel value \n",
    "brightness = 10 \n",
    "# Adjusts the contrast by scaling the pixel values by 2.3 \n",
    "contrast = 2.3  \n",
    "enhanced_img = cv2.addWeighted(img, contrast, np.zeros(img.shape, img.dtype), 0, brightness) \n",
    "  \n",
    "\n",
    "cv2.imshow('Enhanced image (brightness and contrast)', enhanced_img)\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.figure().set_figwidth(10)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(enhanced_img.ravel(),256,[0,256]) \n",
    "plt.show() \n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f0ce5-3a0b-45df-ae2b-c79d670e3bbe",
   "metadata": {},
   "source": [
    "> Another method for adjusting the brightness and contrast of an image is to use the cv2.convertScaleAbs() function, which allows you to adjust the brightness and contrast using a combination of scaling and shifting the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77614b-f47b-4404-8944-55f4a2d29ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'batman1.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "# Adjust the brightness and contrast  \n",
    "# g(i,j)=α⋅f(i,j)+β \n",
    "# control Contrast by 1.5 \n",
    "alpha = 1.5  \n",
    "# control brightness by 50 \n",
    "beta = 10 \n",
    "enhanced_img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "\n",
    "cv2.imshow('Enhanced image (brightness and contrast)', enhanced_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb1924",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.9.1a Histogram Equalization </strong></font></h3>\n",
    "\n",
    "This method usually increases the global contrast of many images, especially when the image is represented by a narrow range of intensity values. Through this adjustment, the intensities can be better distributed on the histogram utilizing the full range of intensities evenly. This allows for areas of lower local contrast to gain a higher contrast. Histogram equalization accomplishes this by effectively spreading out the highly populated intensity values which are used to degrade image contrast. Source: https://en.wikipedia.org/wiki/Histogram_equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c489e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram Equalization\n",
    "import cv2 \n",
    "img = cv2.imread('valley.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "equ = cv2.equalizeHist(img)\n",
    "\n",
    "cv2.imshow('valley', img)\n",
    "cv2.imshow('hist equ', equ)\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.figure().set_figwidth(10)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(equ.ravel(),256,[0,256]) \n",
    "plt.show() \n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8075f-6a73-440b-b995-95c05f201615",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.9.2 Sharpening images</strong></font></h3>\n",
    "\n",
    "Sharpening is the process of enhancing the edges and fine details in an image to make it appear sharper and more defined. It is important because it can help to bring out the details and features in an image, making it more visually appealing and easier to understand. Sharpening can be used to correct blur or softness in an image and can be applied using a variety of techniques.\n",
    "\n",
    "One common method for sharpening images using OpenCV and Python is to use the `cv2.filter2D()` function, which convolves the image with a kernel. The kernel can be designed to enhance the edges in the image, resulting in a sharper image.\n",
    "\n",
    "More information can be found here: https://www.geeksforgeeks.org/python-opencv-filter2d-function/\n",
    "\n",
    "Here is an example of how to sharpen an image using the `cv2.filter2D()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fd96a-7b40-4af9-8ef9-2a35e41bedbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'menu.png')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('Original image', img)\n",
    "\n",
    "# Create the sharpening kernel \n",
    "kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) \n",
    "\n",
    "# Create the blurring kernel \n",
    "#kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) \n",
    "#kernel = kernel / 9\n",
    "\n",
    "# Create the edge detection kernel Sobel \n",
    "#kernel = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]) \n",
    "\n",
    "# Create the edge detection kernel Laplacian of Gaussian (LOG) \n",
    "# note sum of kernal values is zero\n",
    "#kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]) \n",
    "\n",
    "# Sharpen the image with the kernel\n",
    "# Value -1 represents that the resulting image will have same depth as the source image.\n",
    "sharpened_img = cv2.filter2D(img, -1, kernel) \n",
    "\n",
    "cv2.imshow('Sharpened Image', sharpened_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de95138-909c-4891-a6b6-2b77813e7db4",
   "metadata": {},
   "source": [
    "Another method for sharpening images is to use the cv2.Laplacian() function, which calculates the Laplacian of an image and returns the result as a sharpened image.\n",
    "More info: https://docs.opencv.org/3.4/d5/db5/tutorial_laplace_operator.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ff30c-4cc4-4922-bd6b-c671f12bb796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'menu.png')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "  \n",
    "# Sharpen the image \n",
    "sharpened_img = cv2.Laplacian(img, cv2.CV_64F) \n",
    "\n",
    "cv2.imshow('Sharpened Image', sharpened_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a16e8-a634-47b1-99fd-3fe5cbee4e4c",
   "metadata": {},
   "source": [
    "> In this example, the Laplacian operator calculates the sharpened image. You can adjust the depth of the output image using the cv2.CV_64F parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad9774",
   "metadata": {},
   "source": [
    "> The code below can be used to load simple images from the digit dataset and then understand the filtering opertion by applying different types of kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "print(digits.images[1])\n",
    "plt.matshow(digits.images[1], cmap='gray');\n",
    "\n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "img = digits.images[1]\n",
    "# Create the sharpening kernel \n",
    "#kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) \n",
    "\n",
    "# Create the blurring kernel \n",
    "kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) \n",
    "kernel = kernel / 9\n",
    "\n",
    "# Create the edge detection kernel Laplacian of Gaussian (LOG) \n",
    "# note sum of kernal values is zero\n",
    "#kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]) \n",
    "\n",
    "# Create the edge detection kernel Sobel \n",
    "#kernel = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]) \n",
    "\n",
    "print(sharpened_img)\n",
    "sharpened_img = cv2.filter2D(img, -1, kernel) \n",
    "\n",
    "plt.matshow(sharpened_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406f28c-9a34-461c-83f5-2a5195583ce9",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.9.3 Removing noise from images</strong></font></h3>\n",
    "\n",
    "Noise reduction is the process of removing or reducing unwanted noise or artifacts from an image. It is important because it can improve the visual quality and clarity of the image and make it easier to analyze or process using computer algorithms. Noise can be introduced into an image due to a variety of factors and can degrade its quality. There are several techniques for reducing noise, including using filters such as the median filter or the Gaussian filter. It is important to apply noise reduction judiciously to avoid blur or loss of detail in the image.\n",
    "\n",
    "One common method for removing noise from images using OpenCV and Python is to use a median filter. The median filter works by replacing each pixel in the image with the median value of a set of neighboring pixels. This can help to smooth out noise and reduce artifacts in the image.\n",
    "\n",
    "Here is an example of how to remove noise from an image using the cv2.medianBlur() function in OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cabc1-777e-4c4b-8b0c-1fce3bbd89f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'data', 'noisy_image_2.png')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "  \n",
    "# Remove noise using a median filter \n",
    "#filtered_img = cv2.medianBlur(img, 3) \n",
    "\n",
    "# Remove noise using a Gaussian filter \n",
    "#filtered_img = cv2.GaussianBlur(img, (11, 11), 0) \n",
    "\n",
    "# Remove noise using a flat/box blur or averaging filter \n",
    "filtered_img = cv2.blur(img,(5,5))\n",
    "\n",
    "cv2.imshow('Filtered Image', filtered_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf35c6a-e349-491f-9649-1c6548467aa5",
   "metadata": {},
   "source": [
    "In this example, the `cv2.medianBlur()` function is used to apply a median filter to the image. The 3 parameter specifies the size of the kernel to use for the filter. You can adjust the kernel size to achieve the desired level of noise reduction.\n",
    "\n",
    "Another method for removing noise from images is to use a Gaussian filter, which uses a weighted average of neighboring pixels to smooth out noise and reduce artifacts. You can use the `cv2.GaussianBlur()` function to apply a Gaussian filter to an image in OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_figwidth(10)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(filtered_img.ravel(),256,[0,256]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997cd73-23bf-4583-9db3-68795d583bb3",
   "metadata": {},
   "source": [
    "> Also, the  `cv2.GaussianBlur()` function is used to apply a Gaussian filter to the image. The (11, 11) parameter specifies the size of the kernel to use for the filter, and the 0 parameter specifies the standard deviation of the Gaussian function. You can adjust these parameters to achieve the desired level of noise reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9faed12-7720-4dcf-9899-b5c9f1eb2312",
   "metadata": {},
   "source": [
    "For more information visit: https://www.geeksforgeeks.org/image-enhancement-techniques-using-opencv-python/\n",
    "\n",
    "**It is important to note that we can add different type of noise (such as, gaussian , salt-pepper , poisson and speckle noise) into an image. There is function called random_noise() from the scikit-image package. It has several builtin noise patterns. However, we are exploring it in this module.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99185a1-b38f-4de7-a935-589bbcf9ef5c",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.11 Blurring</strong></font></h2>\n",
    "\n",
    "**Goal:**\n",
    "    Blur images with various low pass filters\n",
    "    \n",
    "Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise. It actually removes high frequency content (eg: noise, edges) from the image. So edges are blurred a little bit in this operation (there are also blurring techniques which don't blur the edges). OpenCV provides four main types of blurring techniques.\n",
    "1. Averaging\n",
    "2. Gaussian Blurring\n",
    "3. Median Blurring\n",
    "4. Bilateral Filtering\n",
    "    \n",
    "For more information vivst [Smoothing Images](https://docs.opencv.org/3.4/d4/d13/tutorial_py_filtering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb50bdf-00b7-4348-aa81-b00f2c0ce33d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'data', 'lena_noisy.png')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Classical Blur\n",
    "k_size = 3\n",
    "blurred_img = cv2.blur (img, (k_size, k_size))\n",
    "\n",
    "# Gaussian Blur\n",
    "gauss_blur = cv2.GaussianBlur (img, (k_size, k_size), 5)\n",
    "\n",
    "# Median blur\n",
    "median_blur = cv2.medianBlur (img, k_size)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Noisy\", img)\n",
    "cv2.imshow (\"Box Blurred Legends\", blurred_img)\n",
    "cv2.imshow (\"Gaussina Blur\", gauss_blur)\n",
    "cv2.imshow (\"Median Blur\", median_blur)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdd36c",
   "metadata": {},
   "source": [
    "<font color='blue'>**Image Quality Evaluation: Peak signal-to-noise ratio (PSNR)**\n",
    "\n",
    "PSNR is the one of the most commonly metric in evaluating the quality of an image. PSNR calculates the ratio between the maximum possible power of an image and the power of corrupting noise that affects the quality of its representation. To estimate the PSNR of an image, it is necessary to compare that image to an ideal clean image with the maximum possible power.\n",
    "    \n",
    "More info: https://www.geeksforgeeks.org/python-peak-signal-to-noise-ratio-psnr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join ('..', 'data', 'lena_ori.png')\n",
    "img_o = cv2.imread (image_path)\n",
    "\n",
    "psnr1 = cv2.PSNR(img_o, blurred_img)\n",
    "psnr2 = cv2.PSNR(img_o, gauss_blur)\n",
    "psnr3 = cv2.PSNR(img_o, median_blur)\n",
    "\n",
    "print(psnr1, 'in dB in Box Blur')\n",
    "print(psnr2, 'in dB in Gauss Blur')\n",
    "print(psnr3, 'in dB in Median Blur')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb82c5-033b-40ea-ad0a-fd95deb09b7a",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.12 Object Tracking</strong></font></h2>\n",
    "Now that we know how to convert a BGR image to HSV, we can use this to extract a colored object. In HSV, it is easier to represent a color than in BGR color-space. In our application, we will try to extract a blue colored object. So here is the method:\n",
    "\n",
    "- Take each frame of the video\n",
    "- Convert from BGR to HSV color-space\n",
    "- We threshold the HSV image for a range of blue color\n",
    "- Now extract the blue object alone, we can do whatever we want on that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f12e9-2e00-42d5-a095-04359902bbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#cap = cv.VideoCapture(0)\n",
    "video_path = os.path.join('..', 'data', 'BR99.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "while(1):\n",
    "    # for one frame only; delete the line below for whole video\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    # Convert BGR to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # define range of blue color in HSV\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    \n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "    \n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    \n",
    "    cv2.imshow('original',frame)\n",
    "    cv2.imshow('hsv',hsv)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 113: # 113 is the ascii code for q\n",
    "        break\n",
    "    \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba427c-dd52-4dd3-8413-d0b2ee6db1c2",
   "metadata": {},
   "source": [
    "This is the simplest method in object tracking. Once you learn functions of contours, you can do plenty of things like find the centroid of an object and use it to track the object, draw diagrams just by moving your hand in front of a camera, and other fun stuff.\n",
    "\n",
    "**How to find HSV values to track?**\n",
    "This is a common question found in stackoverflow.com. It is very simple and you can use the same function, cv.cvtColor(). Instead of passing an image, you just pass the BGR values you want. For example, to find the HSV value of Green, try the following commands in a Python terminal:\n",
    "```python\n",
    "green = np.uint8([[[0,255,0 ]]])\n",
    "hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)\n",
    "print( hsv_green )\n",
    "```\n",
    "```bash\n",
    "    [[[ 60 255 255]]]\n",
    "```\n",
    "Now you take [H-10, 100,100] and [H+10, 255, 255] as the lower bound and upper bound respectively. Apart from this method, you can use any image editing tools like GIMP or any online converters to find these values, but don't forget to adjust the HSV ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa02523",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = np.uint8([[[0,0,255 ]]])\n",
    "hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)\n",
    "print(green)\n",
    "print( hsv_green )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230b3c5-6aea-46f0-aa91-c8b7dfbea8ad",
   "metadata": {},
   "source": [
    "> Follow the below link to find the HSV values for any given color:\n",
    "[here](https://www.tydac.ch/color/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b5a3e-50ee-4d70-bf6e-b1762e2d689d",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.13 Thresholding</strong></font></h2>\n",
    "**Goal:**\n",
    "    In this tutorial, you will learn simple thresholding and adaptive thresholding.\n",
    "    \n",
    "<h3><font color=#4296f5><strong>1.13.1 Simple Thresholding</strong></font></h3>   \n",
    "\n",
    "Here, the matter is straight-forward. For every pixel, the same threshold value is applied. If the pixel value is smaller than the threshold, it is set to 0, otherwise it is set to a maximum value. The function cv.threshold is used to apply the thresholding. The first argument is the source image, which should be a grayscale image. The second argument is the threshold value which is used to classify the pixel values. The third argument is the maximum value which is assigned to pixel values exceeding the threshold. OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type cv.THRESH_BINARY. All simple thresholding types are:\n",
    "\n",
    "- cv.THRESH_BINARY\n",
    "- cv.THRESH_BINARY_INV\n",
    "- cv.THRESH_TRUNC\n",
    "- cv.THRESH_TOZERO\n",
    "- cv.THRESH_TOZERO_INV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35204d-7df0-4f72-a583-8615d9cec555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'data', 'elephant.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "# Global threshold\n",
    "ret, thresh = cv2.threshold (img_gray, 90, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Could be used in image segmentation and detection!!!\n",
    "gray_thresh = cv2.blur (thresh, (5, 5))\n",
    "ret, thresh_2 = cv2.threshold (gray_thresh, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Grayed Image!\", img_gray)\n",
    "cv2.imshow (\"1st Threshold applied\", thresh)\n",
    "cv2.imshow (\"2nd Threshold applied\", thresh_2)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7ab78-bdef-4512-83db-a7fad03b4049",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.13.2 Adaptive Thresholding</strong></font></h3> \n",
    "In the previous section, we used one global value as a threshold. But this might not be good in all cases, e.g. if an image has different lighting conditions in different areas. In that case, adaptive thresholding can help. Here, the algorithm determines the threshold for a pixel based on a small region around it. So we get different thresholds for different regions of the same image which gives better results for images with varying illumination.\n",
    "\n",
    "In addition to the parameters described above, the method cv.adaptiveThreshold takes three input parameters:\n",
    "\n",
    "The adaptiveMethod decides how the threshold value is calculated:\n",
    "\n",
    "- cv.ADAPTIVE_THRESH_MEAN_C: The threshold value is the mean of the neighbourhood area minus the constant C.\n",
    "- cv.ADAPTIVE_THRESH_GAUSSIAN_C: The threshold value is a gaussian-weighted sum of the neighbourhood values minus the constant C.\n",
    "The blockSize determines the size of the neighbourhood area and C is a constant that is subtracted from the mean or weighted sum of the neighbourhood pixels.\n",
    "\n",
    "For more information visit [Image Thresholding](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082218f-8b9b-4f5a-9140-7e91eead1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'data', 'elephant.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Global threshold\n",
    "ret, simple_thresh = cv2.threshold (img_gray, 90, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Adaptive threshold\n",
    "adaptive_thresh = cv2.adaptiveThreshold (img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Gray Image!\", img_gray)\n",
    "cv2.imshow (\"Adaptive Threshold Applied\", adaptive_thresh)\n",
    "cv2.imshow (\"Simple Threshold Applied\", simple_thresh)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8844a2-6378-4a7c-8f5c-3e05c6d194e1",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.14 Edge Detection</strong></font></h2>\n",
    "\n",
    "Canny Edge Detection is a popular edge detection algorithm. It was developed by John F. Canny.\n",
    "\n",
    "It is a multi-stage algorithm and we will go through each stages.\n",
    "\n",
    "1. Noise Reduction\n",
    "2. Finding Intensity Gradient of the Image\n",
    "3. Non-maximum Suppression\n",
    "4. Hysteresis Thresholding\n",
    "\n",
    "OpenCV puts all the above in single function, cv.Canny(). We will see how to use it. First argument is our input image. Second and third arguments are our minVal and maxVal respectively. Fourth argument is aperture_size. It is the size of Sobel kernel used for find image gradients. By default it is 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af377e3-037d-45cf-a200-b1c003ac165f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = os.path.join ('..', 'data', 'menu.png')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Canny\n",
    "image_edge = cv2.Canny (img, 100, 200)\n",
    "\n",
    "# Dialate\n",
    "image_edge_dilate = cv2.dilate (image_edge, np.ones ((3, 3), dtype=np.int8))\n",
    "\n",
    "# Erode\n",
    "image_edge_erode = cv2.erode (image_edge_dilate, np.ones ((3, 3), dtype=np.int8))\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Image\", img)\n",
    "cv2.imshow (\"Edges\", image_edge)\n",
    "cv2.imshow (\"Dilated\", image_edge_dilate)\n",
    "cv2.imshow (\"Erode\", image_edge_erode)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a99c17-073e-4d4e-9444-1f324633bf32",
   "metadata": {},
   "source": [
    "**Erosion and Dilation** are part of Morphological transformation that is normally performed on binary images. \n",
    "For more information about dilation and erosion refer to this [link](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee1a34-ed39-4283-82fb-b030956c57a6",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.15 Contours</strong></font></h2>\n",
    "<h3><font color=#4296f5><strong>1.15.1 What are contours?</strong></font></h3> \n",
    "Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.\n",
    "\n",
    "For better accuracy, use binary images. So before finding contours, first apply threshold or canny edge detection.\n",
    "Since OpenCV 3.2, findContours() no longer modifies the source image but returns a modified image as the first of three return parameters.\n",
    "In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black.\n",
    "\n",
    "Let's see how to find contours of a binary image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb0905-01d6-41ef-8bd9-0a61c1a5c4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Kind of an object detector :)\n",
    "image_path = os.path.join ('..', 'data', 'birds.jpg')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# here the birds are blabk with white wky. But object to be found should be white with black background.\n",
    "# So, we need to perform inverse threshold\n",
    "ret, thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "contours, hierarchy = cv2.findContours (thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contours_num = 0\n",
    "\n",
    "for cnt in contours:\n",
    "    #print (cv2.contourArea(cnt))\n",
    "    # Removing noise\n",
    "    if cv2.contourArea(cnt) > 200:\n",
    "        cv2.drawContours (img, cnt, -1, (0, 255, 0), 1)\n",
    "        # Count the counturs!!!\n",
    "        # Drawing bounding boxes around the objects\n",
    "        x1, y1, width, height = cv2.boundingRect (cnt)\n",
    "\n",
    "        cv2.rectangle (img, (x1, y1), (x1+ width, y1 + height), (0, 255, 0), 2)\n",
    "        contours_num += 1\n",
    "\n",
    "print (f\"Number of contours: {contours_num}\")\n",
    "\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Image\", img)\n",
    "cv2.imshow (\"GRAY image\", img_gray)\n",
    "cv2.imshow (\"Inverse Threshold\", thresh)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4657f-f97f-41f3-9148-35ff15c90921",
   "metadata": {},
   "source": [
    "See, there are three arguments in cv.findContours() function, first one is source image, second is contour retrieval mode, third is contour approximation method. And it outputs the contours and hierarchy. contours is a Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n",
    "\n",
    "<h3><font color=#4296f5><strong>1.15.2 How to draw the contours?</strong></font></h3> \n",
    "To draw the contours, ```cv.drawContours``` function is used. It can also be used to draw any shape provided you have its boundary points. Its first argument is source image, second argument is the contours which should be passed as a Python list, third argument is index of contours (useful when drawing individual contour. To draw all contours, pass -1) and remaining arguments are color, thickness etc.\n",
    "\n",
    "- To draw all the contours in an image:\n",
    "```python\n",
    "cv.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "```\n",
    "- To draw an individual contour, say 4th contour:\n",
    "```python\n",
    "cv.drawContours(img, contours, 3, (0,255,0), 3)\n",
    "```\n",
    "- But most of the time, below method will be useful:\n",
    "```python\n",
    "cnt = contours[4]\n",
    "cv.drawContours(img, [cnt], 0, (0,255,0), 3)\n",
    "```\n",
    ">**Note**<br>\n",
    "Last two methods are same, but when you go forward, you will see last one is more useful.\n",
    "\n",
    "<h3><font color=#4296f5><strong>1.15.3 Contour Approximation Method</strong></font></h3> \n",
    "This is the third argument in ```cv.findContours``` function. What does it denote actually?\n",
    "\n",
    "Above, we told that contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this contour approximation method.\n",
    "\n",
    "If you pass **cv.CHAIN_APPROX_NONE**, all the boundary points are stored. But actually do we need all the points? For eg, you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we need just two end points of that line. This is what **cv.CHAIN_APPROX_SIMPLE** does. It removes all redundant points and compresses the contour, thereby saving memory.\n",
    "\n",
    "Below image of a rectangle demonstrate this technique. Just draw a circle on all the coordinates in the contour array (drawn in blue color). First image shows points I got with **cv.CHAIN_APPROX_NONE** (734 points) and second image shows the one with **cv.CHAIN_APPROX_SIMPLE** (only 4 points). See, how much memory it saves!!!\n",
    "\n",
    "> For more information, visit the following [link](https://docs.opencv.org/3.4/d3/d05/tutorial_py_table_of_contents_contours.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be1506-58fe-4d5d-989c-c9ba85dc0633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
