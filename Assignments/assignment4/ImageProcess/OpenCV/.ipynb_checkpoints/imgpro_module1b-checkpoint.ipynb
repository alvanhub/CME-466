{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42158dcb",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>2-D Spatial Filtering</strong></font></h2>\n",
    "\n",
    "Image filtering can be done in two domains.\n",
    "\n",
    "<h3><font color=#4296f5><strong>Spatial domain</strong></font></h3>\n",
    "Filtering is performed using convolution operations. A filter matrix, often referred to as convolution kernel, is scanned across the image. The output pixel value is the weighted sum of the input pixels within the area of the filter matrix (local operation).\n",
    "\n",
    "<h3><font color=#4296f5><strong>Frequency (or Spectral) domain</strong></font></h3>\n",
    "Apply discrete transforms (such as, FFT, DCT, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb655df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blurring and PSNR\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# read image\n",
    "image_path = os.path.join('..', 'imgs', 'test_pat.tif')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Classical Blur\n",
    "k_size = 5\n",
    "\n",
    "# Example 1: Default boundary handling (usually reflects)\n",
    "blurred_img_d = cv2.blur (img, (k_size, k_size), 0, borderType=cv2.BORDER_DEFAULT)\n",
    "\n",
    "# Example 2: Replicate border (good for avoiding dark edges)\n",
    "blurred_img_r = cv2.blur (img, (k_size, k_size), 0, borderType=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# Example 3: Constant padding (zeros)\n",
    "blurred_img_z = cv2.blur (img, (k_size, k_size), 0, borderType=cv2.BORDER_CONSTANT)\n",
    "\n",
    "\n",
    "psnr1 = cv2.PSNR(img, blurred_img_d)\n",
    "psnr2 = cv2.PSNR(img, blurred_img_r)\n",
    "psnr3 = cv2.PSNR(img, blurred_img_z)\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original\", img)\n",
    "cv2.imshow (\"Default\", blurred_img_d)\n",
    "cv2.imshow (\"Replicate\", blurred_img_r)\n",
    "cv2.imshow (\"Zero\", blurred_img_z)\n",
    "\n",
    "print(psnr1, 'in dB in default')\n",
    "print(psnr2, 'in dB in replicate')\n",
    "print(psnr3, 'in dB in zero')\n",
    "\n",
    "cv2.waitKey(0) \n",
    "# Release the VideoCapture object and close all windows\n",
    "cv2.destroyAllWindows()# Example 1: Default boundary handling (usually reflects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5468443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blurring more...same as denoising\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# read image\n",
    "#image_path = os.path.join('..', 'imgs', 'test_pat.tif')\n",
    "image_path = os.path.join('..', 'imgs', 'ckt_noisy.tif')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Classical Blur\n",
    "k_size = 3\n",
    "blurred_img = cv2.blur (img, (k_size, k_size))\n",
    "\n",
    "# Gaussian Blur\n",
    "gauss_blur = cv2.GaussianBlur (img, (k_size, k_size), 5)\n",
    "\n",
    "# Median blur\n",
    "median_blur = cv2.medianBlur (img, k_size)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original\", img)\n",
    "cv2.imshow (\"Classical Blur\", blurred_img)\n",
    "cv2.imshow (\"Gaussina Blur\", gauss_blur)\n",
    "cv2.imshow (\"Median Blur\", median_blur)\n",
    "\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Gaussian noise\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    # Create an array of random noise\n",
    "    noise = np.zeros(image.shape, np.int16)\n",
    "    cv2.randn(noise, mean, sigma)\n",
    "    \n",
    "    # Add noise to original image\n",
    "    noisy_img = cv2.add(image, noise, dtype=cv2.CV_8U)\n",
    "    return noisy_img\n",
    "\n",
    "image_path = os.path.join('..', 'imgs', 'FIP.bmp')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "noisy = add_gaussian_noise(img)\n",
    "cv2.imshow (\"Original\", img)\n",
    "cv2.imshow('Gaussian Noise', noisy)\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.figure().set_figwidth(15)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(noisy.ravel(),256,[0,256]) \n",
    "plt.show() \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebf768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising salt and pepper\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'imgs', 'noisy_image_2.png')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "  \n",
    "# Remove noise using a median filter \n",
    "#filtered_img = cv2.medianBlur(img, 3) \n",
    "\n",
    "# Remove noise using a Gaussian filter \n",
    "#filtered_img = cv2.GaussianBlur(img, (11, 11), 0) \n",
    "\n",
    "# Remove noise using a flat/box blur or averaging filter \n",
    "filtered_img = cv2.blur(img,(3,3))\n",
    "\n",
    "cv2.imshow('Filtered Image', filtered_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_figwidth(10)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(img.ravel(),256,[0,256]) \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(filtered_img.ravel(),256,[0,256]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e063a",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.9.2 Sharpening images</strong></font></h3>\n",
    "\n",
    "Sharpening is the process of enhancing the edges and fine details in an image to make it appear sharper and more defined. It is important because it can help to bring out the details and features in an image, making it more visually appealing and easier to understand. Sharpening can be used to correct blur or softness in an image and can be applied using a variety of techniques.\n",
    "\n",
    "One common method for sharpening images using OpenCV and Python is to use the `cv2.filter2D()` function, which convolves the image with a kernel. The kernel can be designed to enhance the edges in the image, resulting in a sharper image.\n",
    "\n",
    "More information can be found here: https://www.geeksforgeeks.org/python-opencv-filter2d-function/\n",
    "\n",
    "Here is an example of how to sharpen an image using the `cv2.filter2D()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b84353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge detection and sharpening\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "#image_path = os.path.join('..', 'imgs', 'menu.png')\n",
    "#image_path = os.path.join('..', 'imgs', 'FIP.bmp')\n",
    "image_path = os.path.join('..', 'imgs', 'skeleton.tif')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('Original image', img)\n",
    "\n",
    "# Create the sharpening kernel \n",
    "kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) \n",
    "\n",
    "# Create the blurring kernel \n",
    "#kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) \n",
    "#kernel = kernel / 9\n",
    "\n",
    "# Create the edge detection kernel Sobel \n",
    "#kernel = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]) \n",
    "\n",
    "# Create the edge detection kernel Laplacian of Gaussian (LOG) \n",
    "# note sum of kernal values is zero\n",
    "#kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]) \n",
    "\n",
    "# Sharpen the image with the kernel\n",
    "# Value -1 represents that the resulting image will have same depth as the source image.\n",
    "sharpened_img = cv2.filter2D(img, -1, kernel) \n",
    "\n",
    "cv2.imshow('Sharpened Image', sharpened_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0981855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use digits dataset from scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.images[1])\n",
    "plt.matshow(digits.images[1], cmap='gray');\n",
    "\n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "img = digits.images[1]\n",
    "# Create the sharpening kernel \n",
    "#kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) \n",
    "\n",
    "# Create the blurring kernel \n",
    "kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) \n",
    "kernel = kernel / 9\n",
    "\n",
    "# Create the edge detection kernel Laplacian of Gaussian (LOG) \n",
    "# note sum of kernal values is zero\n",
    "#kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]) \n",
    "\n",
    "# Create the edge detection kernel Sobel \n",
    "#kernel = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]) \n",
    "\n",
    "print(sharpened_img)\n",
    "sharpened_img = cv2.filter2D(img, -1, kernel) \n",
    "\n",
    "plt.matshow(sharpened_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os\n",
    "  \n",
    "# Read the image\n",
    "image_path = os.path.join('..', 'imgs', 'menu.png')\n",
    "#image_path = os.path.join('..', 'imgs', 'build.tif')\n",
    "#image_path = os.path.join('..', 'imgs', 'FIP.bmp')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Displaying the image\n",
    "cv2.imshow('image', img)\n",
    "  \n",
    "# Edges in the image \n",
    "sharpened_img = cv2.Laplacian(img, cv2.CV_64F, 2) \n",
    "\n",
    "cv2.imshow('Detected edges', sharpened_img)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canny  edge detection with Erode and Dilation\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = os.path.join('..', 'imgs', 'build.tif')\n",
    "#image_path = os.path.join('..', 'imgs', 'FIP.bmp')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "# Canny\n",
    "image_edge = cv2.Canny (img, 100, 200)\n",
    "\n",
    "# Dialate\n",
    "image_edge_dilate = cv2.dilate (image_edge, np.ones ((3, 3), dtype=np.int8))\n",
    "\n",
    "# Erode\n",
    "image_edge_erode = cv2.erode (image_edge_dilate, np.ones ((3, 3), dtype=np.int8))\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Image\", img)\n",
    "cv2.imshow (\"Canny Edges\", image_edge)\n",
    "cv2.imshow (\"Dilated\", image_edge_dilate)\n",
    "cv2.imshow (\"Erode\", image_edge_erode)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d6dc",
   "metadata": {},
   "source": [
    "**Erosion and Dilation** are part of Morphological transformation that is normally performed on binary images. \n",
    "For more information about dilation and erosion refer to this [link](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab0617",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.13 Thresholding</strong></font></h2>\n",
    "    \n",
    "<h3><font color=#4296f5><strong>1.13.1 Simple Thresholding</strong></font></h3>   \n",
    "\n",
    "Here, the matter is straight-forward. For every pixel, the same threshold value is applied. If the pixel value is smaller than the threshold, it is set to 0, otherwise it is set to a maximum value. The function cv.threshold is used to apply the thresholding. The first argument is the source image, which should be a grayscale image. The second argument is the threshold value which is used to classify the pixel values. The third argument is the maximum value which is assigned to pixel values exceeding the threshold. OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type cv.THRESH_BINARY. All simple thresholding types are:\n",
    "\n",
    "- cv.THRESH_BINARY\n",
    "- cv.THRESH_BINARY_INV\n",
    "- cv.THRESH_TRUNC\n",
    "- cv.THRESH_TOZERO\n",
    "- cv.THRESH_TOZERO_INV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'imgs', 'elephant.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "# Global threshold\n",
    "ret, thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Could be used in image segmentation and detection!!!\n",
    "gray_thresh = cv2.blur (thresh, (5, 5))\n",
    "ret, thresh_2 = cv2.threshold (gray_thresh, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Grayed Image\", img_gray)\n",
    "cv2.imshow (\"1st Threshold applied\", thresh)\n",
    "cv2.imshow (\"2nd Threshold applied\", thresh_2)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c33e2",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>1.13.2 Adaptive Thresholding</strong></font></h3> \n",
    "In the previous section, we used one global value as a threshold. But this might not be good in all cases, e.g. if an image has different lighting conditions in different areas. In that case, adaptive thresholding can help. Here, the algorithm determines the threshold for a pixel based on a small region around it. So we get different thresholds for different regions of the same image which gives better results for images with varying illumination.\n",
    "\n",
    "In addition to the parameters described above, the method cv.adaptiveThreshold takes three input parameters:\n",
    "\n",
    "The adaptiveMethod decides how the threshold value is calculated:\n",
    "\n",
    "- cv.ADAPTIVE_THRESH_MEAN_C: The threshold value is the mean of the neighbourhood area minus the constant C.\n",
    "- cv.ADAPTIVE_THRESH_GAUSSIAN_C: The threshold value is a gaussian-weighted sum of the neighbourhood values minus the constant C.\n",
    "The blockSize determines the size of the neighbourhood area and C is a constant that is subtracted from the mean or weighted sum of the neighbourhood pixels.\n",
    "\n",
    "For more information visit [Image Thresholding](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7448426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_path = os.path.join ('..', 'imgs', 'elephant.jpg')\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Global threshold\n",
    "ret, simple_thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# see the difference between normal and inverse threshold\n",
    "#ret, simple_thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Adaptive threshold\n",
    "adaptive_thresh = cv2.adaptiveThreshold (img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 5)\n",
    "# change the parameters and see the effects\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Gray Image!\", img_gray)\n",
    "cv2.imshow (\"Adaptive Threshold Applied\", adaptive_thresh)\n",
    "cv2.imshow (\"Simple Threshold Applied\", simple_thresh)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49021ac9",
   "metadata": {},
   "source": [
    "<h2><font color=#f542f2><strong>1.15 Contours</strong></font></h2>\n",
    "<h3><font color=#4296f5><strong>1.15.1 What are contours?</strong></font></h3> \n",
    "Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.\n",
    "\n",
    "For better accuracy, use binary images. So before finding contours, first apply threshold or canny edge detection.\n",
    "Since OpenCV 3.2, findContours() no longer modifies the source image but returns a modified image as the first of three return parameters.\n",
    "In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black.\n",
    "\n",
    "Let's see how to find contours of a binary image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contours \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Kind of an object detector :)\n",
    "image_path = os.path.join ('..', 'imgs', 'birds.jpg')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# here the birds are blabk with white wky. But object to be found should be white with black background.\n",
    "# So, we need to perform Inverse Threshold\n",
    "ret, thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "contours, hierarchy = cv2.findContours (thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contours_num = 0\n",
    "\n",
    "for cnt in contours:\n",
    "    #print (cv2.contourArea(cnt))\n",
    "    # Removing noise\n",
    "    if cv2.contourArea(cnt) > 150:\n",
    "        cv2.drawContours (img, cnt, -1, (0, 255, 0), 1)\n",
    "        # Count the counturs!!!\n",
    "        # Drawing bounding boxes around the objects\n",
    "        x1, y1, width, height = cv2.boundingRect (cnt)\n",
    "\n",
    "        cv2.rectangle (img, (x1, y1), (x1+ width, y1 + height), (0, 255, 0), 2)\n",
    "        contours_num += 1\n",
    "\n",
    "print (f\"Number of contours: {contours_num}\")\n",
    "\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Image\", img)\n",
    "cv2.imshow (\"GRAY image\", img_gray)\n",
    "cv2.imshow (\"Inverse Threshold\", thresh)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c5bda",
   "metadata": {},
   "source": [
    "See, there are three arguments in cv.findContours() function, first one is source image, second is contour retrieval mode, third is contour approximation method. And it outputs the contours and hierarchy. contours is a Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n",
    "\n",
    "<h3><font color=#4296f5><strong>1.15.2 How to draw the contours?</strong></font></h3> \n",
    "To draw the contours, ```cv.drawContours``` function is used. It can also be used to draw any shape provided you have its boundary points. Its first argument is source image, second argument is the contours which should be passed as a Python list, third argument is index of contours (useful when drawing individual contour. To draw all contours, pass -1) and remaining arguments are color, thickness etc.\n",
    "\n",
    "- To draw all the contours in an image:\n",
    "```python\n",
    "cv.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "```\n",
    "- To draw an individual contour, say 4th contour:\n",
    "```python\n",
    "cv.drawContours(img, contours, 3, (0,255,0), 3)\n",
    "```\n",
    "- But most of the time, below method will be useful:\n",
    "```python\n",
    "cnt = contours[4]\n",
    "cv.drawContours(img, [cnt], 0, (0,255,0), 3)\n",
    "```\n",
    ">**Note**<br>\n",
    "Last two methods are same, but when you go forward, you will see last one is more useful.\n",
    "\n",
    "<h3><font color=#4296f5><strong>1.15.3 Contour Approximation Method</strong></font></h3> \n",
    "This is the third argument in ```cv.findContours``` function. What does it denote actually?\n",
    "\n",
    "Above, we told that contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this contour approximation method.\n",
    "\n",
    "If you pass **cv.CHAIN_APPROX_NONE**, all the boundary points are stored. But actually do we need all the points? For eg, you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we need just two end points of that line. This is what **cv.CHAIN_APPROX_SIMPLE** does. It removes all redundant points and compresses the contour, thereby saving memory.\n",
    "\n",
    "Below image of a rectangle demonstrate this technique. Just draw a circle on all the coordinates in the contour array (drawn in blue color). First image shows points I got with **cv.CHAIN_APPROX_NONE** (734 points) and second image shows the one with **cv.CHAIN_APPROX_SIMPLE** (only 4 points). See, how much memory it saves!!!\n",
    "\n",
    "> For more information, visit the following [link](https://docs.opencv.org/3.4/d3/d05/tutorial_py_table_of_contents_contours.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell count or coin count application \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Kind of an object detector :)\n",
    "image_path = os.path.join ('..', 'imgs', 'cells.jpg')\n",
    "#image_path = os.path.join ('..', 'imgs', 'coins.jpg')\n",
    "\n",
    "img = cv2.imread (image_path)\n",
    "\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "\n",
    "img_gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "#img_grayb = cv2.GaussianBlur (img_gray, (5, 5), 5)\n",
    "# see the effect of gaussian blur\n",
    "\n",
    "#edge = cv2.Canny (img_gray, 100, 200)\n",
    "\n",
    "\n",
    "ret, thresh = cv2.threshold (img_gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "#ret, thresh = cv2.threshold (img_gray, 70, 255, cv2.THRESH_BINARY_INV) #change the threshold value\n",
    "\n",
    "contours, hierarchy = cv2.findContours (thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#contours, hierarchy = cv2.findContours (thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#contours, hierarchy = cv2.findContours (edge, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contours_num = 0\n",
    "\n",
    "for cnt in contours:\n",
    "    #print (cv2.contourArea(cnt))\n",
    "    # Removing noise\n",
    "    if cv2.contourArea(cnt) > 150: #start from 150....upto 2000\n",
    "        cv2.drawContours (img, cnt, -1, (0, 255, 0), 1)\n",
    "        # Count the counturs!!!\n",
    "        # Drawing bounding boxes around the objects\n",
    "        x1, y1, width, height = cv2.boundingRect (cnt)\n",
    "\n",
    "        cv2.rectangle (img, (x1, y1), (x1+ width, y1 + height), (0, 255, 0), 2)\n",
    "        contours_num += 1\n",
    "\n",
    "print (f\"Number of contours: {contours_num}\")\n",
    "\n",
    "\n",
    "# Visualization\n",
    "cv2.imshow (\"Original Image\", img)\n",
    "cv2.imshow (\"GRAY image\", img_gray)\n",
    "cv2.imshow (\"Inverse Threshold\", thresh)\n",
    "\n",
    "cv2.waitKey (0)\n",
    "cv2.destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fd37c",
   "metadata": {},
   "source": [
    "<h3><font color=#4296f5><strong>Drawing circles</strong></font></h3> \n",
    "\n",
    "To draw a circle, you need its center coordinates and radius. We will draw a circle inside the rectangle drawn above.\n",
    "See code: cv2.circle(img,(447,63), 63, (0,0,255), -1)\n",
    "https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_r = 10\n",
    "max_r = 20\n",
    "\n",
    "circles = cv2.HoughCircles(        \n",
    "    img_gray,  # source image\n",
    "    cv2.HOUGH_GRADIENT,  # type of detection\n",
    "    1,\n",
    "    40,\n",
    "    param1=50,\n",
    "    param2=30,\n",
    "    minRadius=min_r*2,  # minimal radius\n",
    "    maxRadius=max_r*2,  # max radius\n",
    "    )\n",
    "\n",
    "print(circles.shape)\n",
    "img_copy = img.copy()\n",
    "\n",
    "for detected_circle in circles[0]:\n",
    "    x_coor, y_coor, detected_radius = detected_circle\n",
    "    cv2.circle(img_copy, (int(x_coor), int(y_coor)), int(detected_radius), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Detected_Hough\", img_copy)\n",
    "cv2.waitKey(0) # Wait indefinitely for a key press\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19c07f",
   "metadata": {},
   "source": [
    "***Question:*** How to detect overlapping objects?\n",
    "\n",
    "***Answer:*** Apply morphological operations (like, erode and dilate) to separate touching boundaries. For example, erode can shrink objects, potentially breaking the connection between overlapping ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce020f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
